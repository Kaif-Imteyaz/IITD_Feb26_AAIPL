{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480066b8",
   "metadata": {},
   "source": [
    "<h1 align='center'>Synthetic Data Generation and Unsloth Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8e0a-e95f-4f46-840f-944bd7335754",
   "metadata": {},
   "source": [
    "## ğŸ“š Table of Contents:\n",
    "\n",
    "- [Synthetic Data Kit: Data Generation](#synthetic-data-generation)\n",
    "- [Unsloth: Fine-Tuning and saving the model](#fine-tuning)\n",
    "\n",
    "## Synthetic Data Generation\n",
    "\n",
    "In this section, we use the CLI from synthetic-data-kit to generate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roro3n17eek",
   "metadata": {},
   "source": [
    "### Testing Synthetic Data Kit Command\n",
    "\n",
    "Please make sure you are running vllm by opening a terminal and typing `vllm serve Unsloth/Llama-3.3-70B-Instruct   --port 8001   --max-model-len 48000   --gpu-memory-utilization 0.85`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sv060626q8r",
   "metadata": {},
   "source": [
    "### Exploring Synthetic Data Kit CLI\n",
    "\n",
    "This command displays the help menu for the `synthetic-data-kit` CLI tool, showing available commands:\n",
    "- **system-check**: Verify LLM provider server is running\n",
    "- **ingest**: Parse documents (PDF, HTML, YouTube, etc.) into clean text\n",
    "- **create**: Generate synthetic content (Q&A pairs, instructions, etc.) using LLM\n",
    "- **curate**: Filter and clean generated content based on quality scores\n",
    "- **save-as**: Convert data to different formats (fine-tuning format, JSON, etc.)\n",
    "- **server**: Launch web interface for the toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f33f54be-101e-4e96-b892-b05ed5a08a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1msynthetic-data-kit [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      " A toolkit for preparing synthetic datasets for fine-tuning LLMs                \n",
      "                                                                                \n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m Options \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-config\u001b[0m              \u001b[1;32m-c\u001b[0m      \u001b[1;33mPATH\u001b[0m  Path to configuration file               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          \u001b[1;33m    \u001b[0m  Install completion for the current       \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                     shell.                                   \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             \u001b[1;33m    \u001b[0m  Show completion for the current shell,   \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                     to copy it or customize the              \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                     installation.                            \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        \u001b[1;33m    \u001b[0m  Show this message and exit.              \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36msystem-check \u001b[0m\u001b[1;36m \u001b[0m Check if the selected LLM provider's server is running.       \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mingest       \u001b[0m\u001b[1;36m \u001b[0m Parse documents (PDF, HTML, YouTube, DOCX, PPT, TXT) into     \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m              \u001b[0m clean text.                                                   \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mcreate       \u001b[0m\u001b[1;36m \u001b[0m Generate content from text using local LLM inference.         \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mcurate       \u001b[0m\u001b[1;36m \u001b[0m Clean and filter content based on quality.                    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36msave-as      \u001b[0m\u001b[1;36m \u001b[0m Convert to different formats for fine-tuning.                 \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mserver       \u001b[0m\u001b[1;36m \u001b[0m Start a web interface for the Synthetic Data Kit.             \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lou0vs3mbib",
   "metadata": {},
   "source": [
    "### Verifying LLM Server Status\n",
    "\n",
    "This command checks if the vLLM server is running and accessible at `http://localhost:8001/v1`. It displays:\n",
    "- Server status and endpoint\n",
    "- Available models (here: Unsloth/Llama-3.3-70B-Instruct)\n",
    "- Model configuration (max context length: 48000 tokens)\n",
    "\n",
    "The system is configured to use the vLLM provider as specified in `tutorial_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a6d30e-d3cc-43e5-b1dd-189d393aa8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "\u001b[1;34mEnvironment variable check:\u001b[0m\n",
      "API_ENDPOINT_KEY: Not found\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[?25l\u001b[31mL vLLM server is not available at \u001b[0m\u001b[4;94mhttp://localhost:8001/v1\u001b[0m\n",
      "\u001b[2KError: \u001b[1;35mHTTPConnectionPool\u001b[0m\u001b[1m(\u001b[0m\u001b[33mhost\u001b[0m=\u001b[32m'localhost'\u001b[0m, \u001b[33mport\u001b[0m=\u001b[1;36m8001\u001b[0m\u001b[1m)\u001b[0m: Max retries exceeded \n",
      "with url: \u001b[35m/v1/\u001b[0m\u001b[95mmodels\u001b[0m \u001b[1m(\u001b[0mCaused by \n",
      "\u001b[1;35mNewConnectionError\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32murllib3.connection.HTTPConnection\u001b[0m\u001b[32m object at \u001b[0m\n",
      "\u001b[32m0x7f38d5e24b00\u001b[0m\u001b[32m>\u001b[0m\u001b[32m: Failed to establish a new connection: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mErrno 111\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Connection \u001b[0m\n",
      "\u001b[32mrefused'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
      "\u001b[2Kmâ ‹\u001b[0m Checking vLLM server at http://localhost:8001/v1...\n",
      "\u001b[33mTo start the server, run:\u001b[0m\n",
      "\u001b[2K\u001b[1;34mvllm serve Unsloth/Llama-\u001b[0m\u001b[1;36m3.3\u001b[0m\u001b[1;34m-70B-Instruct --port \u001b[0m\u001b[1;36m8001\u001b[0m\n",
      "\u001b[2K\u001b[32mâ ‹\u001b[0m Checking vLLM server at http://localhost:8001/v1...v1...\u001b[0m\n",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c tutorial_config.yaml system-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lj8z1mjus4",
   "metadata": {},
   "source": [
    "### Creating Project Directory Structure\n",
    "\n",
    "This command creates a well-organized directory structure for the logical reasoning project:\n",
    "- `sources/`: Store original source documents (PDFs, etc.)\n",
    "- `data/input/`: Input files for processing\n",
    "- `data/parsed/`: Parsed text files after document ingestion\n",
    "- `data/generated/`: Generated synthetic Q&A pairs\n",
    "- `data/curated/`: Quality-filtered data after curation\n",
    "- `data/final/`: Final formatted data ready for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "289ebeda-4d12-4466-a6df-4a82bd4a175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -c 'mkdir -p logical_reasoning/{sources,data/{input,parsed,generated,curated,final}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0vu978x4i9hb",
   "metadata": {},
   "source": [
    "### Navigating to Project Directory\n",
    "\n",
    "Changes the current working directory to `logical_reasoning/` where all subsequent operations will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "264477d5-7d25-4fac-99a5-5497d7dcd753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/AAIPL/logical_reasoning/logical_reasoning/logical_reasoning\n"
     ]
    }
   ],
   "source": [
    "cd logical_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ukh0uhxa8ca",
   "metadata": {},
   "source": [
    "### Downloading Source Documents\n",
    "\n",
    "Downloads two PDF documents related to logical reasoning and liar/truth puzzles:\n",
    "1. \"Logical Reasoning\" textbook from CSU Sacramento\n",
    "2. \"Liar and Truth Teller Puzzles\" from UMass\n",
    "\n",
    "These documents will serve as the knowledge base for generating synthetic training data. The `-q` flag runs wget in quiet mode, and `--show-progress` displays a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fa11bee-b9b0-470a-875c-673bc88d3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logical-reasoning-1 100%[===================>]   5.52M  1.55MB/s    in 3.8s    \n",
      "Liar_Truth.pdf      100%[===================>] 327.61K  --.-KB/s    in 0.1s    \n"
     ]
    }
   ],
   "source": [
    "!wget -P sources/ -q --show-progress   \"https://www.csus.edu/faculty/d/dowden/_internal/_documents/logical-reasoning-12.pdf\"   \"https://people.cs.umass.edu/~pthomas/solutions/Liar_Truth.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcvmiim3d49",
   "metadata": {},
   "source": [
    "### Copying Source Files to Input Directory\n",
    "\n",
    "Copies all downloaded source documents from `sources/` to `data/input/` to prepare them for the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26d79b79-1f40-4ede-bc54-60496ccf65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp sources/* data/input/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ttgi76hqiv",
   "metadata": {},
   "source": [
    "### Ingesting and Parsing Documents\n",
    "\n",
    "This command processes the PDF files in `data/input/` using the synthetic-data-kit's **ingest** command:\n",
    "- Extracts text content from PDFs\n",
    "- Cleans and normalizes the text\n",
    "- Saves parsed text files to `data/parsed/`\n",
    "\n",
    "The output shows successful processing of 2 PDF files (Liar_Truth.pdf and logical-reasoning-12.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c71729fe-a9f2-495c-9ef5-6f58bcaa1232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/input/\u001b[0m\n",
      "\u001b[33mNo supported files found in .\u001b[0m\u001b[35m/data/input/\u001b[0m\n",
      "\u001b[33mSupported extensions: .pdf, .html, .htm, .docx, .pptx, .txt\u001b[0m\n",
      "\u001b[32mâœ… All files processed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit ingest ./data/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s722mc1b399",
   "metadata": {},
   "source": [
    "### Generating Synthetic Q&A Pairs\n",
    "\n",
    "This command uses the synthetic-data-kit's **create** command to generate Q&A pairs from the parsed text:\n",
    "- Reads parsed text files from `data/parsed/`\n",
    "- Uses the vLLM provider with Llama-3.3-70B-Instruct model\n",
    "- Generates 50 Q&A pairs per file (`--num-pairs 50`)\n",
    "- Type is set to `qa` for question-answer pair generation\n",
    "- Outputs are saved to `data/generated/`\n",
    "\n",
    "The process chunks the text and generates questions with corresponding answers. This took about 10 minutes for the full run. Use `--verbose` flag to see detailed progress or reduce `--num-pairs` for faster testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe68a35-cbdd-453b-93fd-0d85e5489cea",
   "metadata": {},
   "source": [
    "Note: This will take about 10 minutes, set `--verbose` flag to see progress or reduce the `num-pairs` for a faster test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c382c960-75dc-4f7c-8450-bf53d89fd904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/AAIPL\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af209d32-0a99-4365-bee0-18a14af2c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[32mğŸ”— Using vllm provider\u001b[0m\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/logical_reasoning/data/parsed/\u001b[0m\u001b[34m for qa generation\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m2\u001b[0m\u001b[34m qa files to process\u001b[0m\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "L Using vllm provider\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Processing 1 chunks to generate QA pairs...\n",
      "Batch processing complete.                                                      \n",
      "Generated 12 QA pairs total (requested: 50)\n",
      "Saving result to data/generated/Liar_Truth_qa_pairs.json\n",
      "Successfully wrote test file to data/generated/test_write.json\n",
      "Successfully wrote result to data/generated/Liar_Truth_qa_pairs.json\n",
      "\u001b[32mâœ“ Liar_Truth.txt\u001b[0m\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "L Using vllm provider\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Processing 352 chunks to generate QA pairs...\n",
      "Batch processing complete.                                                      \n",
      "Generated 50 QA pairs total (requested: 50)\n",
      "Saving result to data/generated/logical-reasoning-12_qa_pairs.json\n",
      "Successfully wrote test file to data/generated/test_write.json\n",
      "Successfully wrote result to data/generated/logical-reasoning-12_qa_pairs.json\n",
      "\u001b[32mâœ“ logical-reasoning-\u001b[0m\u001b[1;36m12.\u001b[0m\u001b[32mtxt\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mContent Generation Summary \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mqa\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n",
      "Total files: \u001b[1;36m2\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[32mFailed: \u001b[0m\u001b[1;36m0\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[32mâœ… All files processed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c tutorial_config.yaml create ./logical_reasoning/data/parsed/ --type qa --num-pairs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr9f7y5tqur",
   "metadata": {},
   "source": [
    "### Curating and Quality Filtering\n",
    "\n",
    "This command uses the **curate** function to filter generated Q&A pairs based on quality:\n",
    "- Evaluates each Q&A pair using quality metrics\n",
    "- Filters pairs with quality score above threshold (7.0/10)\n",
    "- Removes low-quality, inconsistent, or malformed pairs\n",
    "- Saves curated data to `data/curated/`\n",
    "\n",
    "This ensures only high-quality synthetic data is used for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabcce1-73f4-453b-a12e-7374f044462b",
   "metadata": {},
   "source": [
    "Note: This will also take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1e185b1-4357-4bb6-86b9-2e645c7956f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/AAIPL\n"
     ]
    }
   ],
   "source": [
    "!pwd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44510f47-9cbd-4890-984f-3980c145e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[32mğŸ”— Using vllm provider\u001b[0m\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/generated/\u001b[0m\u001b[34m for curation\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m3\u001b[0m\u001b[34m JSON files to curate\u001b[0m\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Processing 3 batches of QA pairs...\n",
      "Batch processing complete.                                                      \n",
      "Rated 8 QA pairs\n",
      "Retained 3 pairs (threshold: 7.0)\n",
      "Average score: 4.8\n",
      "\u001b[32mâœ“ Liar_Truth_qa_pairs.json\u001b[0m\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Processing 10 batches of QA pairs...\n",
      "Batch processing complete.                                                      \n",
      "Rated 46 QA pairs\n",
      "Retained 32 pairs (threshold: 7.0)\n",
      "Average score: 7.2\n",
      "\u001b[32mâœ“ logical-reasoning-12_qa_pairs.json\u001b[0m\n",
      "\u001b[31mâœ— test_write.json: No QA pairs found in the input file\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mCuration Summary \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mthreshold: \u001b[0m\u001b[1;36m7.0\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n",
      "Total files: \u001b[1;36m3\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[31mFailed: \u001b[0m\u001b[1;36m1\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[33mâš ï¸  Completed with \u001b[0m\u001b[1;36m1\u001b[0m\u001b[33m errors\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c tutorial_config.yaml curate ./data/generated/ --threshold 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awlg3cppl1t",
   "metadata": {},
   "source": [
    "### Converting to Fine-Tuning Format\n",
    "\n",
    "This command uses the **save-as** function to convert curated Q&A pairs to fine-tuning format:\n",
    "- Reads curated JSON files from `data/curated/`\n",
    "- Converts to format `ft` (fine-tuning format with messages structure)\n",
    "- Outputs are saved to `data/final/` with proper conversation format\n",
    "- The resulting format is compatible with standard fine-tuning pipelines\n",
    "\n",
    "Successfully converted 2 files to fine-tuning format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67ed73a8-7f3e-4a6d-9bdb-ba190b8b1bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/curated/\u001b[0m\u001b[34m for format conversion to ft\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m2\u001b[0m\u001b[34m JSON files to convert to ft format\u001b[0m\n",
      "\u001b[32mâœ“ Liar_Truth_qa_pairs_cleaned.json\u001b[0m\n",
      "\u001b[32mâœ“ logical-reasoning-12_qa_pairs_cleaned.json\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mFormat Conversion Summary \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mft, json\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n",
      "Total files: \u001b[1;36m2\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[32mFailed: \u001b[0m\u001b[1;36m0\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[32mâœ… All files converted successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit save-as ./data/curated/ --format ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3akj2fu3yr8",
   "metadata": {},
   "source": [
    "### Loading and Converting Data to HuggingFace Dataset\n",
    "\n",
    "This cell performs comprehensive data processing:\n",
    "\n",
    "1. **Finding Files**: Locates all JSON files in `data/final/` directory\n",
    "2. **Loading Data**: Reads each JSON file containing fine-tuning formatted data\n",
    "3. **Format Conversion**: Extracts user and assistant messages from the fine-tuning format\n",
    "4. **Structuring Conversations**: Creates a standardized conversation format with role-content pairs\n",
    "5. **Creating Dataset**: Converts the processed data into a HuggingFace Dataset object\n",
    "\n",
    "The output shows 74 total conversations were successfully loaded and formatted. The preview displays a sample conversation showing either a knight-and-knave logic puzzle with its solution or a deduction puzzle about the Logical Reasoning book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3dbbd1b2-a4f2-46b7-bd01-6841abde7637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Total conversations: 35\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"content\": \"Three people, A, B, and C, are standing in a line. A says, \\\"B is a liar.\\\" B says, \\\"C is a liar.\\\" C says, \\\"A and B are both liars.\\\" Everyone either tells the truth or always lies. Who is a truth\\u2011teller and who is a liar?\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"Let\\u2019s test each person\\u2019s statement step by step. 1. Assume A is truthful. Then B must be a liar (because A says B is a liar). If B is a liar, B\\u2019s statement that C is a liar is false, so C is truthful. But C says both A and B are liars. That would make A a liar, contradicting our assumption that A is truthful. Therefore A cannot be truthful. 2. Assume A is a liar. Then A\\u2019s statement \\\"B is a liar\\\" is false, so B is truthful. Since B is truthful, B\\u2019s statement \\\"C is a liar\\\" is true, so C is a liar. 3. Check C\\u2019s statement: C is a liar, so the statement \\\"A and B are both liars\\\" must be false. A is a liar and B is truthful, so indeed it is false that both are liars. The contradiction is resolved. Conclusion: A is a liar, B is truthful, and C is a liar.\",\n",
      "      \"role\": \"assistant\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "data_dir = \"./data/final\"  # Change this to your data directory\n",
    "\n",
    "# ===== STEP 1: Find all FT files =====\n",
    "data_path = Path(data_dir)\n",
    "ft_files = glob.glob(str(data_path / \"*.json\"))\n",
    "\n",
    "# ===== STEP 2: Load and convert all files =====\n",
    "all_data = []\n",
    "\n",
    "for file_path in ft_files:\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        ft_data = json.load(f)\n",
    "    \n",
    "    # Convert each item\n",
    "    for item in ft_data:\n",
    "        if 'messages' not in item:\n",
    "            continue\n",
    "        \n",
    "        # Extract only user and assistant messages\n",
    "        conversation = []\n",
    "        for msg in item['messages']:\n",
    "            if msg['role'] == 'user' or msg['role'] == 'assistant':\n",
    "                conversation.append({\n",
    "                    \"role\": msg['role'],\n",
    "                    \"content\": msg['content']\n",
    "                })\n",
    "        \n",
    "        # Add to our data if we have at least one exchange\n",
    "        if len(conversation) > 0:\n",
    "            all_data.append({\n",
    "                \"conversations\": conversation\n",
    "            })\n",
    "\n",
    "print(f\"\\nğŸ¯ Total conversations: {len(all_data)}\")\n",
    "\n",
    "# ===== STEP 3: Create HuggingFace Dataset =====\n",
    "dataset = Dataset.from_list(all_data)\n",
    "\n",
    "# ===== STEP 4: Preview the data =====\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ec326-9902-49db-aee0-b84acfdfb2cb",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "### Note: Please remember to shutdown the vLLM instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5edd3d-b80c-4d61-b6b4-ad650f6bba96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dulvo8ocyrb",
   "metadata": {},
   "source": [
    "### Importing Standard Libraries\n",
    "\n",
    "Imports essential Python libraries for fine-tuning:\n",
    "- `os`, `json`, `glob`: File system operations and JSON handling\n",
    "- `torch`: PyTorch deep learning framework\n",
    "- `shutil`: File operations\n",
    "- `Path`: Path manipulation\n",
    "- `Dataset`: HuggingFace datasets library for data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77d5db74-5070-45b7-bdae-f93d50909b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwk3zcrzft",
   "metadata": {},
   "source": [
    "### Importing Unsloth and Training Libraries\n",
    "\n",
    "Imports specialized libraries for efficient fine-tuning:\n",
    "- `FastLanguageModel` from Unsloth: Optimized model loading and training\n",
    "- `get_chat_template`, `standardize_sharegpt`, `train_on_responses_only`: Chat formatting utilities\n",
    "- `SFTConfig`, `SFTTrainer`: Supervised fine-tuning configuration and trainer from TRL\n",
    "- `DataCollatorForSeq2Seq`: Handles batching and padding for sequence-to-sequence training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8adaf850-eb8a-476e-8d09-34c807c92e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 02-14 11:47:36 [__init__.py:225] Automatically detected platform rocm.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305661d0-5103-412c-9f33-7ad61cc288b3",
   "metadata": {},
   "source": [
    "## Setup Unsloth model and tokenizer for ROCm without bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "erpk4j0opb6",
   "metadata": {},
   "source": [
    "### Loading Llama-3.3-70B Model with LoRA\n",
    "\n",
    "This cell sets up the model for efficient fine-tuning on AMD ROCm hardware:\n",
    "\n",
    "**Model Configuration:**\n",
    "- Model: Llama-3.3-70B-Instruct (70 billion parameters)\n",
    "- Data type: bfloat16 for ROCm compatibility\n",
    "- No quantization (load_in_4bit=False) to avoid bitsandbytes dependency\n",
    "- Max sequence length: 1024 tokens\n",
    "\n",
    "**LoRA (Low-Rank Adaptation) Configuration:**\n",
    "- Rank (r): 64 - Higher rank for the large 70B model\n",
    "- Target modules: All attention and MLP layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj)\n",
    "- LoRA alpha: 64\n",
    "- Dropout: 0 (no dropout)\n",
    "- Gradient checkpointing: \"unsloth\" for memory efficiency\n",
    "\n",
    "LoRA enables efficient fine-tuning by only training small adapter layers instead of the entire 70B model, making it feasible to train on a single AMD MI300X GPU with 192GB HBM3 memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5dffe96-b007-4220-b7b7-e40e219b267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/.no_exist/cc89b3e7fd423253264883a80a4fa5abc619649f/adapter_config.json'\n",
      "[2026-02-14 11:48:47] ERROR file_download.py:1556: Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/.no_exist/cc89b3e7fd423253264883a80a4fa5abc619649f/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/.no_exist/cc89b3e7fd423253264883a80a4fa5abc619649f/adapter_config.json'\n",
      "[2026-02-14 11:48:47] ERROR file_download.py:1556: Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/.no_exist/cc89b3e7fd423253264883a80a4fa5abc619649f/adapter_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gpt_Oss patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Gpt_Oss does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-14 11:48:48] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fc013a84814fa38614bcd54c3c316f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded: GPT OSS 20B (bfloat16, ROCm compatible)\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = torch.bfloat16  # Explicit bfloat16 for ROCm\n",
    "load_in_4bit = False  \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gpt-oss-20b-BF16\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # Explicit for ROCm\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded: GPT OSS 20B (bfloat16, ROCm compatible)\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # Higher rank for 70B model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9exgyip7y8f",
   "metadata": {},
   "source": [
    "### Preparing Dataset with Chat Template\n",
    "\n",
    "This cell formats the dataset for fine-tuning:\n",
    "\n",
    "**Steps:**\n",
    "1. **Set Chat Template**: Applies Llama-3.1 chat template formatting\n",
    "2. **Configure Padding**: Sets pad token to eos token if not already set\n",
    "3. **Format Conversations**: The `formatting_prompts_func` function:\n",
    "   - Takes raw conversations from the dataset\n",
    "   - Applies the chat template to format them properly\n",
    "   - Validates conversation structure (list of dicts with role/content)\n",
    "   - Filters out malformed conversations\n",
    "4. **Standardize Format**: Uses `standardize_sharegpt` to normalize the data structure\n",
    "5. **Apply Formatting**: Maps the formatting function across all examples\n",
    "6. **Remove Empty**: Filters out any empty or invalid formatted texts\n",
    "\n",
    "The output shows 74 valid examples were successfully prepared. A sample of the formatted text is displayed, showing the proper Llama-3.1 chat template structure with system, user, and assistant headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "496f9c09-609c-41f7-a7a7-c1f942405a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Preparing dataset for training...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'GPT-OSS-20'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ”§ Preparing dataset for training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Set chat template\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer = \u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGPT-OSS-20\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Ensure pad token is set\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth/chat_templates.py:2212\u001b[39m, in \u001b[36mget_chat_template\u001b[39m\u001b[34m(tokenizer, chat_template, mapping, map_eos_token, system_message)\u001b[39m\n\u001b[32m   2208\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(chat_template) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   2209\u001b[39m     \u001b[38;5;66;03m# For changing system message later\u001b[39;00m\n\u001b[32m   2210\u001b[39m     type_chat_template = chat_template.lower()\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m     chat_template, stop_word, yes_map_eos_token, ollama_modelfile = \u001b[43mCHAT_TEMPLATES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2214\u001b[39m     \u001b[38;5;66;03m# Check mapping to eos_token\u001b[39;00m\n\u001b[32m   2215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m map_eos_token \u001b[38;5;129;01mand\u001b[39;00m yes_map_eos_token: map_eos_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'GPT-OSS-20'"
     ]
    }
   ],
   "source": [
    "\"\"\"Prepare dataset with proper chat template and tensor compatibility\"\"\"\n",
    "print(\"ğŸ”§ Preparing dataset for training...\")\n",
    "\n",
    "# Set chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"GPT-OSS-20\")\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Formatting function that ensures proper tensor conversion\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        # Ensure conversation is in correct format\n",
    "        if isinstance(convo, list) and all(isinstance(msg, dict) for msg in convo):\n",
    "            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "        else:\n",
    "            print(f\"âš ï¸  Skipping malformed conversation: {type(convo)}\")\n",
    "            continue\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"âœ… Prepared {len(dataset)} valid examples for training\")\n",
    "\n",
    "# Show sample\n",
    "if len(dataset) > 0:\n",
    "    print(f\"ğŸ“ Sample formatted text:\")\n",
    "    print(dataset[\"text\"][0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otx9lfwgfmi",
   "metadata": {},
   "source": [
    "### Training the Model with ROCm-Optimized Settings\n",
    "\n",
    "This cell configures and executes the fine-tuning process:\n",
    "\n",
    "**Training Configuration (SFTConfig):**\n",
    "- **Batch size**: 64 per device - leveraging the AMD MI300X's massive 192GB HBM3 memory\n",
    "- **Gradient accumulation**: 1 step\n",
    "- **Warmup**: 5 steps\n",
    "- **Epochs**: 1 full pass through the dataset\n",
    "- **Learning rate**: 1e-4\n",
    "- **Optimizer**: adamw_8bit for memory efficiency\n",
    "- **Precision**: bf16 (bfloat16) for ROCm\n",
    "- **Gradient checkpointing**: Enabled for memory efficiency\n",
    "\n",
    "**Special Training Mode:**\n",
    "Uses `train_on_responses_only` to compute loss only on the assistant's responses, not on the user's questions. This focuses the model on learning to generate accurate answers rather than memorizing the input format.\n",
    "\n",
    "**Key Features:**\n",
    "- DataCollatorForSeq2Seq handles variable-length sequences with proper padding\n",
    "- No packing to preserve conversation structure\n",
    "- Single dataloader worker for ROCm stability\n",
    "- Gradient checkpointing via Unsloth for memory optimization\n",
    "\n",
    "The model is then trained on the 74 logical reasoning conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "144231a7-313f-4db9-8f02-025148666732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 35. Reducing num_proc to 35 for dataset of size 35.\n",
      "[2026-02-14 11:57:29] WARNING arrow_dataset.py:3114: num_proc must be <= 35. Reducing num_proc to 35 for dataset of size 35.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d74faba24849b58a636189164807b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=35):   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 35. Reducing num_proc to 35 for dataset of size 35.\n",
      "[2026-02-14 11:57:41] WARNING arrow_dataset.py:3114: num_proc must be <= 35. Reducing num_proc to 35 for dataset of size 35.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb955701167e431f8c50c49da4c8b61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=35):   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.673100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.673100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Train model with ROCm-optimized settings\"\"\"\n",
    "# Ensure tokenizer has proper padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Setup trainer with ROCm-friendly settings and proper data handling\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=32,  # ğŸš€ MI300X can handle this with 192GB HBM3!\n",
    "        gradient_accumulation_steps=2,   # Effective batch size = 32*2 = 64\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",  # Pure torch optimizer\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"logical_reasoning_rocm_outputs\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,  # Remove unused columns to avoid tensor issues\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,  # Single worker for ROCm stability\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train only on responses\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"user\",  # Try simpler matching first\n",
    "    response_part=\"assistant\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlv8ydhyokk",
   "metadata": {},
   "source": [
    "### Saving the Fine-Tuned Model\n",
    "\n",
    "This cell saves the trained model in two formats:\n",
    "\n",
    "1. **LoRA Adapters** (`logical_reasoning_rocm_lora/`):\n",
    "   - Saves only the trained LoRA adapter weights (lightweight, ~few hundred MB)\n",
    "   - Can be loaded later with the base model\n",
    "   - Useful for sharing or deploying with the original base model\n",
    "\n",
    "2. **Merged Model** (`logical_reasoning_rocm_merged/`):\n",
    "   - Merges LoRA adapters back into the base model\n",
    "   - Creates a standalone model with all weights\n",
    "   - Saved in 16-bit precision for better quality\n",
    "   - Ready for immediate inference without loading adapters\n",
    "\n",
    "Both formats include the tokenizer configuration. The merged model is production-ready and can be used directly for generating answers to logical reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d05799dd-25f8-4a03-8bb0-6b91d5d6dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ SAVING ROCM-TRAINED MODEL\n",
      "âœ… LoRA adapters saved to: logical_reasoning_rocm_lora\n",
      "ğŸ”„ Saving merged model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignored error while writing commit hash to /root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/refs/main: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/refs/main'.\n",
      "[2026-02-14 11:58:38] WARNING _snapshot_download.py:300: Ignored error while writing commit hash to /root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/refs/main: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub/models--unsloth--gpt-oss-20b-BF16/refs/main'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found cache directory /root/.cache/huggingface/hub, but lack R/W/X permissions. Cannot use cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:21<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:45<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/AAIPL/logical_reasoning_rocm_merged`\n",
      "âœ… Merged model saved to: logical_reasoning_rocm_merged\n",
      "\n",
      "ğŸ‰ ROCM MODEL READY!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save the trained model\"\"\"\n",
    "print(\"\\nğŸ’¾ SAVING ROCM-TRAINED MODEL\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "lora_path = \"logical_reasoning_rocm_lora\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"âœ… LoRA adapters saved to: {lora_path}\")\n",
    "\n",
    "# Save merged model\n",
    "merged_path = \"logical_reasoning_rocm_merged\"\n",
    "print(\"ğŸ”„ Saving merged model...\")\n",
    "model.save_pretrained_merged(merged_path, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"âœ… Merged model saved to: {merged_path}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ROCM MODEL READY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee904c-4ee0-45b4-9a06-9d7f01b1b255",
   "metadata": {},
   "source": [
    "### Testing the Fine-Tuned Model\n",
    "You can do a quick test to check that the model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23dfae80-9635-4d62-b72a-b5671d7c62b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: A says 'B is a knave.' B says 'A and I are different types.' What are A and B?\n",
      "\n",
      "Answer: We need to solve a logic puzzle: A says \"B is a knave.\" B says \"A and I are different types.\" Determine who is knight or knave. Knights always tell truth; knaves always lie. We need to find consistent assignments.\n",
      "\n",
      "Letâ€™s analyze.\n",
      "\n",
      "Let A be Knight or Knave. Let B be Knight or Knave.\n",
      "\n",
      "A says: \"B is a knave.\" So statement S_A: B is knave.\n",
      "\n",
      "If A is Knight, S_A true => B is knave.\n",
      "\n",
      "If A is Knave, S_A false => B is not knave => B is knight.\n",
      "\n",
      "B says: \"A and I are different types.\" Statement S_B: A and B are different types. That is, (A is Knight and B is Knave) or (A is Knave and B is Knight). Equivalent to A != B.\n",
      "\n",
      "If B is Knight, S_B true => A != B.\n",
      "\n",
      "If B is Knave, S_B false => A == B.\n",
      "\n",
      "Now we need to find consistent assignments.\n",
      "\n",
      "Case 1: A Knight. Then B knave (from A's statement). So A Knight, B Knave. Check B's statement: B is knave, so his statement must be false. Statement: A and B are different types. Are they different? A Knight, B Knave => yes, they are different. So statement would be true. But B is knave, cannot say truth. Contradiction. So A cannot be Knight.\n",
      "\n",
      "Case 2: A Knave. Then B Knight (since A's statement false). So A Knave, B Knight. Check B's statement: B Knight, so statement must be true. Statement: A and B are different types. A Knave, B Knight => different. So statement true. Works. So consistent.\n",
      "\n",
      "Thus A is Knave, B is Knight.\n",
      "\n",
      "But we must double-check: Are there any other possibilities? Let's test.\n",
      "\n",
      "Case 3: Suppose A Knight, B Knight? But A's statement would be false because B is not knave. So A cannot be Knight.\n",
      "\n",
      "Case 4: Suppose A Knave, B Knave? Then A's statement false => B is not knave => B Knight. Contradiction. So only solution: A Knave, B Knight.\n",
      "\n",
      "Thus answer: A is a knave, B is a knight.\n",
      "\n",
      "But we might also consider possibility of liar telling false statements but not necessarily always lying? In Knights and Knaves puzzles, kn\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test the fine-tuned model with inference\"\"\"\n",
    "# Switch model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test question - a classic knight/knave logic puzzle\n",
    "test_question = \"A says 'B is a knave.' B says 'A and I are different types.' What are A and B?\"\n",
    "\n",
    "# Format the prompt using the chat template\n",
    "messages = [{\"role\": \"user\", \"content\": test_question}]\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True  # Adds assistant header so model knows to respond\n",
    ")\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,      # Low temperature for more deterministic logical reasoning\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Decode only the generated part (exclude the input prompt)\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9b5d9-a31e-4824-80e4-26b75e68d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97406d4-568e-40b7-84b5-6066d58a8d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88146ed6-7f4e-4f7d-a622-066ef9652d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
